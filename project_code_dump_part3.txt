
📦 Project Structure of: e:\MyProjects\student_performance

📄 .env
📄 .gitignore
📄 Dockerfile
📄 app.py
📁 artifacts/
    📁 2025_06_13T00_11_29Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_34_59Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_42_39Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_48_16Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_49_44Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_50_23Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_52_14Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_54_26Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_55_30Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_transformation/
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_56_13Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_transformation/
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
    📁 2025_06_13T15_56_30Z/
        📁 data_ingestion/
            📁 ingested_data/
                📄 ingested_data.csv
            📁 raw_data/
                📄 raw.csv
        📁 data_transformation/
            📄 x_preprocessor.joblib
            📄 y_preprocessor.joblib
        📁 data_validation/
            📁 reports/
                📄 categorical_report.yaml
                📄 drift_report.yaml
                📄 duplicates_report.yaml
                📄 missing_values_report.yaml
                📄 validation_report.yaml
            📁 validated/
                📄 validated_data.csv
📁 config/
    📄 config.yaml
    📄 params.yaml
    📄 schema.yaml
    📄 templates.yaml
📁 data/
    📁 raw/
        📄 raw.csv
    📁 validated/
        📄 validated_data.csv
📄 debug.py
📁 logs/
    📁 2025_06_13T00_11_29Z/
        📄 2025_06_13T00_11_29Z.log
    📁 2025_06_13T15_34_59Z/
        📄 2025_06_13T15_34_59Z.log
    📁 2025_06_13T15_42_39Z/
        📄 2025_06_13T15_42_39Z.log
    📁 2025_06_13T15_48_16Z/
        📄 2025_06_13T15_48_16Z.log
    📁 2025_06_13T15_49_44Z/
        📄 2025_06_13T15_49_44Z.log
    📁 2025_06_13T15_50_23Z/
        📄 2025_06_13T15_50_23Z.log
    📁 2025_06_13T15_52_14Z/
        📄 2025_06_13T15_52_14Z.log
    📁 2025_06_13T15_54_26Z/
        📄 2025_06_13T15_54_26Z.log
    📁 2025_06_13T15_55_30Z/
        📄 2025_06_13T15_55_30Z.log
    📁 2025_06_13T15_56_13Z/
        📄 2025_06_13T15_56_13Z.log
    📁 2025_06_13T15_56_30Z/
        📄 2025_06_13T15_56_30Z.log
📄 main.py
📁 notebook/
    📄 research.ipynb
📄 project_code_dump_index.txt
📄 project_code_dump_part1.txt
📄 project_code_dump_part2.txt
📄 project_code_dump_part3.txt
📄 project_dump.py
📄 project_template.py
📄 requirements.txt
📁 research/
    📄 research.ipynb
📄 setup.py
📁 src/
    📁 student_performance/
        📄 __init__.py
        📁 components/
            📄 __init__.py
            📄 data_ingestion.py
            📄 data_transformation.py
            📄 data_validation.py
        📁 config/
            📄 __init__.py
            📄 configuration.py
        📁 constants/
            📄 __init__.py
            📄 constants.py
        📁 data_processors/
            📄 __init__.py
            📄 column_math_factory.py
            📄 encoder_factory.py
            📄 imputer_factory.py
            📄 preprocessor_builder.py
            📄 scaler_factory.py
        📁 dbhandler/
            📄 __init__.py
            📄 base_handler.py
            📄 postgres_dbhandler.py
        📁 entity/
            📄 __init__.py
            📄 artifact_entity.py
            📄 config_entity.py
        📁 exception/
            📄 __init__.py
            📄 exception.py
        📁 logging/
            📄 __init__.py
            📄 app_logger.py
        📁 pipeline/
            📄 __init__.py
            📄 training_pipeline.py
        📁 utils/
            📄 __init__.py
            📄 core.py
            📄 timestamp.py
📁 student_data/
    📄 stud.csv
📁 templates/
    📄 index.html

--- CODE DUMP | PART 3 of 3 ---


================================================================================
# PY FILE: src\student_performance\exception\exception.py
================================================================================

import sys
from types import TracebackType
from logging import Logger


class StudentPerformanceError(Exception):
    """
    Custom exception for the Student Performance project.

    Automatically captures:
    - Original exception message
    - Filename and line number from the traceback
    - Logs the formatted error using an injected logger
    """

    def __init__(self, error: Exception, logger: Logger) -> None:
        # Set the core message
        super().__init__(str(error))
        self.message: str = str(error)
        self.logger: Logger = logger

        # Extract traceback info from current exception context
        _, _, tb = sys.exc_info()
        tb: TracebackType | None

        # Safely capture line number and file
        self.line: int | None = tb.tb_lineno if tb and tb.tb_lineno else None
        self.file: str = tb.tb_frame.f_code.co_filename if tb and tb.tb_frame else "Unknown"

        # Log the error using injected logger with traceback
        try:
            self.logger.error(str(self), exc_info=True)
        except Exception as log_error:
            print(f"Logging failed inside StudentPerformanceError: {log_error}")

    def __str__(self) -> str:
        return (
            f"Error occurred in file [{self.file}], "
            f"line [{self.line}], "
            f"message: [{self.message}]"
        )

================================================================================
# PY FILE: src\student_performance\logging\__init__.py
================================================================================

"""
Centralized logger for the student_performance project.

Provides a reusable `logger` instance configured with:
- UTC timestamped log directory and file
- File + stream handlers
- DEBUG level logging by default
"""

import logging
from .app_logger import setup_logger

# Static logger name and level
LOGGER_NAME = "student_performance_logger"
LOG_LEVEL = logging.DEBUG

# Initialize logger once and share across the project
logger = setup_logger(name=LOGGER_NAME, level=LOG_LEVEL)

================================================================================
# PY FILE: src\student_performance\logging\app_logger.py
================================================================================

import logging
import sys
from pathlib import Path
from src.student_performance.constants.constants import LOGS_ROOT
from src.student_performance.utils.timestamp import get_utc_timestamp

def setup_logger(name: str = "app_logger", level: int = logging.DEBUG) -> logging.Logger:
    """
    Set up and return a logger with file and stream handlers, allowing custom log level.

    Args:
        name (str): Name of the logger.
        level (int): Logging level (e.g., logging.DEBUG, logging.INFO).

    Returns:
        logging.Logger: Configured logger instance.
    """
    sys.stdout.reconfigure(encoding="utf-8")
    timestamp = get_utc_timestamp()

    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)
    log_filepath = log_dir / f"{timestamp}.log"

    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        stream_handler.setLevel(level)
        logger.addHandler(stream_handler)

    return logger

================================================================================
# PY FILE: src\student_performance\pipeline\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\pipeline\training_pipeline.py
================================================================================

from src.student_performance.config.configuration import ConfigurationManager
from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.dbhandler.postgres_dbhandler import PostgresDBHandler
from src.student_performance.logging import logger

from src.student_performance.components.data_ingestion import DataIngestion
from src.student_performance.components.data_validation import DataValidation
from src.student_performance.components.data_transformation import DataTransformation


class TrainingPipeline:
    def __init__(self):
        try:
            logger.info("Initializing TrainingPipeline...")
            self.config_manager = ConfigurationManager()

        except Exception as e:
            msg = "Failed to initialize TrainingPipeline."
            raise StudentPerformanceError(e, msg) from e

    def run_pipeline(self):
        try:
            logger.info("========== Training Pipeline Started ==========")

            # Step 1: Setup configurations and database handler
            postgres_config = self.config_manager.get_postgres_handler_config()
            data_ingestion_config = self.config_manager.get_data_ingestion_config()
            postgresdb_handler = PostgresDBHandler(postgres_config=postgres_config)

            # Step 2: Run data ingestion
            data_ingestion = DataIngestion(
                ingestion_config=data_ingestion_config,
                db_handler=postgresdb_handler,
            )
            data_ingestion_artifact = data_ingestion.run_ingestion()
            logger.info(f"Data Ingestion Artifact: {data_ingestion_artifact}")

            # Step 3: Run data validation
            data_validation_config = self.config_manager.get_data_validation_config()
            data_validation = DataValidation(
                validation_config=data_validation_config,
                ingestion_artifact=data_ingestion_artifact,
            )
            data_validation_artifact = data_validation.run_validation()
            logger.info(f"Data Validation Artifact: {data_validation_artifact}")

            # Step 4: Run data transformation (only if validation passed)
            if data_validation_artifact.validation_status:
                data_transformation_config = self.config_manager.get_data_transformation_config()
                data_transformation = DataTransformation(
                    transformation_config=data_transformation_config,
                    validation_artifact=data_validation_artifact
                )
                data_transformation_artifact = data_transformation.run_transformation()
                logger.info(f"Data Transformation Artifact: {data_transformation_artifact}")
            else:
                logger.warning("Data validation failed. Skipping data transformation step.")

            logger.info("========== Training Pipeline Completed ==========")

        except Exception as e:
            msg = "TrainingPipeline failed."
            raise StudentPerformanceError(e, msg) from e

================================================================================
# PY FILE: src\student_performance\utils\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\utils\core.py
================================================================================

import pandas as pd
from pathlib import Path
from box import ConfigBox
from box.exceptions import BoxValueError, BoxTypeError, BoxKeyError
from ensure import ensure_annotations
import yaml
import json
import numpy as np
import pandas as pd
import joblib

from src.student_performance.logging import logger
from src.student_performance.exception.exception import StudentPerformanceError


@ensure_annotations
def read_yaml(path_to_yaml: Path) -> ConfigBox:
    """
    Load a YAML file and return its contents as a ConfigBox for dot-access.

    Raises:
        StudentPerformanceError: If the file is missing, corrupted, or unreadable.
    """
    if not path_to_yaml.exists():
        msg = f"YAML file not found: '{path_to_yaml}'"
        raise StudentPerformanceError(FileNotFoundError(msg), msg)

    try:
        with path_to_yaml.open("r", encoding="utf-8") as file:
            content = yaml.safe_load(file)
    except (BoxValueError, BoxTypeError, BoxKeyError, yaml.YAMLError) as e:
        msg = f"Failed to parse YAML from: '{path_to_yaml.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e
    except Exception as e:
        msg = f"Unexpected error while reading YAML from: '{path_to_yaml.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e

    if content is None:
        msg = f"YAML file is empty or improperly formatted: '{path_to_yaml}'"
        raise StudentPerformanceError(ValueError(msg), msg)

    logger.info(f"YAML successfully loaded from: '{path_to_yaml.as_posix()}'")
    return ConfigBox(content)


@ensure_annotations
def save_to_csv(df: pd.DataFrame, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            df.to_csv(path, index=False)
            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        msg = f"Failed to save CSV to: '{path.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e


@ensure_annotations
def read_csv(filepath: Path) -> pd.DataFrame:
    """
    Read a CSV file into a Pandas DataFrame.

    Raises:
        StudentPerformanceError: If the file is missing, corrupted, or unreadable.
    """
    if not filepath.exists():
        msg = f"CSV file not found: '{filepath}'"
        raise StudentPerformanceError(FileNotFoundError(msg), msg)

    try:
        df = pd.read_csv(filepath)
        logger.info(f"CSV file read successfully from: '{filepath.as_posix()}'")
        return df
    except Exception as e:
        msg = f"Failed to read CSV from: '{filepath.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e

@ensure_annotations
def save_to_yaml(data: dict, *paths: Path, label: str):
    """
    Write a dict out to YAML, always using UTF-8.
    """
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            # Write UTF-8
            with open(path, "w", encoding="utf-8") as file:
                yaml.dump(data, file, sort_keys=False)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        msg = f"Failed to read CSV from: '{path.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e

@ensure_annotations
def save_to_json(data: dict, *paths: Path, label: str):
    try:
        for path in paths:
            path = Path(path)
            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            with open(path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=4)

            logger.info(f"{label} saved to: '{path.as_posix()}'")
    except Exception as e:
        msg = f"Failed to read CSV from: '{path.as_posix()}' — {e}"
        raise StudentPerformanceError(e, msg) from e


@ensure_annotations
def save_object(obj: object, path: Path, label: str):
    """
    Saves a serializable object using joblib to the specified path.

    Args:
        obj (object): The object to serialize.
        path (Path): The path to save the object.
        label (str): Label used for logging context.
    """
    try:
        path = Path(path)
        if not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
        else:
            logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

        joblib.dump(obj, path)
        logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        msg = f"Failed to save {label} to: '{path.as_posix()}'"
        raise StudentPerformanceError(e, logger) from e


@ensure_annotations
def save_array(array: np.ndarray | pd.Series, *paths: Path, label: str):
    """
    Saves a NumPy array or pandas Series to the specified paths in `.npy` format.

    Args:
        array (Union[np.ndarray, pd.Series]): Data to save.
        *paths (Path): One or more file paths.
        label (str): Label for logging.
    """
    try:
        array = np.asarray(array)

        for path in paths:
            path = Path(path)

            if not path.parent.exists():
                path.parent.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created directory for {label}: '{path.parent.as_posix()}'")
            else:
                logger.info(f"Directory already exists for {label}: '{path.parent.as_posix()}'")

            np.save(path, array)
            logger.info(f"{label} saved to: '{path.as_posix()}'")

    except Exception as e:
        msg = f"Failed to save {label} to: '{path.as_posix()}'"
        raise StudentPerformanceError(e, logger) from e


@ensure_annotations
def load_array(path: Path, label: str) -> np.ndarray:
    """
    Loads a NumPy array from the specified `.npy` file path.

    Args:
        path (Path): Path to the `.npy` file.
        label (str): Label for logging.

    Returns:
        np.ndarray: Loaded NumPy array.
    """
    try:
        path = Path(path)

        if not path.exists():
            raise FileNotFoundError(f"{label} file not found at path: '{path.as_posix()}'")

        array = np.load(path)
        logger.info(f"{label} loaded successfully from: '{path.as_posix()}'")
        return array

    except Exception as e:
        msg = f"Failed to load {label} from: '{path.as_posix()}'"
        raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\utils\timestamp.py
================================================================================

from datetime import datetime, timezone

_timestamp_cache: str | None = None

def get_utc_timestamp() -> str:
    format="%Y_%m_%dT%H_%M_%SZ"
    global _timestamp_cache
    if _timestamp_cache is None:
        _timestamp_cache = datetime.now(timezone.utc).strftime(format)
    return _timestamp_cache

================================================================================
# YAML FILE: config\config.yaml
================================================================================

# PostGres configuration
postgres_dbhandler:
  input_data_dir: student_data
  input_data_filename: stud.csv
  dbname: student_performance_db
  table_name: student_scores

# Data ingestion configuration
data_ingestion:
  raw_data_filename: raw.csv
  ingested_data_filename: ingested_data.csv

# Data validation configuration
data_validation:
  validated_data_filename: validated_data.csv
  missing_report_filename: missing_values_report.yaml
  duplicates_report_filename: duplicates_report.yaml
  drift_report_filename: drift_report.yaml
  validation_report_filename: validation_report.yaml
  categorical_report_filename: categorical_report.yaml

# Data transformation configuration
data_transformation:
  x_train_filename: x_train.npy
  y_train_filename: y_train.npy
  x_val_filename: x_val.npy
  y_val_filename: y_val.npy
  x_test_filename: x_test.npy
  y_test_filename: y_test.npy
  x_preprocessor_filename: x_preprocessor.joblib
  y_preprocessor_filename: y_preprocessor.joblib

================================================================================
# YAML FILE: config\params.yaml
================================================================================

# Parameters for drift detection during data validation
validation_params:
  drift_detection:
    enabled: true
    method: ks_test
    p_value_threshold: 0.05

  schema_check:
    enabled: true
    method: hash

transformation_params:
  data_split:
    train_size: 0.6
    val_size: 0.2
    test_size: 0.2
    random_state: 42
    stratify: false

  steps:
    x:
      - encoding
      - standardization
    y:
      - column_math
      - standardization

  methods:
    x:
      encoding:
        method: one_hot
        handle_unknown: ignore

      standardization:
        method: standard_scaler
        with_mean: false
        with_std: true

    y:
      column_math:
        method: mean_of_columns
        output_column: mean_score
        input_column:
          - math_score
          - reading_score
          - writing_score

      standardization:
        method: standard_scaler
        with_mean: true
        with_std: true

================================================================================
# YAML FILE: config\schema.yaml
================================================================================

table_schema:
  student_scores:
    columns:
      id:
        type: "SERIAL PRIMARY KEY"

      gender:
        type: "VARCHAR(10)"
        constraints:
          allowed_values:
            - "male"
            - "female"

      race_ethnicity:
        type: "VARCHAR(20)"
        constraints:
          allowed_values:
            - "group A"
            - "group B"
            - "group C"
            - "group D"
            - "group E"

      parental_level_of_education:
        type: "VARCHAR(50)"
        constraints:
          allowed_values:
            - "some high school"
            - "high school"
            - "associate's degree"
            - "some college"
            - "bachelor's degree"
            - "master's degree"

      lunch:
        type: "VARCHAR(20)"
        constraints:
          allowed_values:
            - "standard"
            - "free/reduced"

      test_preparation_course:
        type: "VARCHAR(20)"
        constraints:
          allowed_values:
            - "none"
            - "completed"

      math_score:
        type: "INT"
        constraints:
          min: 0
          max: 100

      reading_score:
        type: "INT"
        constraints:
          min: 0
          max: 100

      writing_score:
        type: "INT"
        constraints:
          min: 0
          max: 100

validation_schema:
  columns:
    id: int64
    gender: object
    race_ethnicity: object
    parental_level_of_education: object
    lunch: object
    test_preparation_course: object
    math_score: int64
    reading_score: int64
    writing_score: int64

  allowed_values:
    gender:
      - male
      - female
    race_ethnicity:
      - group A
      - group B
      - group C
      - group D
      - group E
    parental_level_of_education:
      - some high school
      - high school
      - some college
      - associate's degree
      - bachelor's degree
      - master's degree
    lunch:
      - standard
      - free/reduced
    test_preparation_course:
      - none
      - completed

target_column:
  - math_score
  - reading_score
  - writing_score

================================================================================
# YAML FILE: config\templates.yaml
================================================================================

validation_report:
  timestamp: ""
  
  validation_status: null
  critical_passed: null
  non_critical_passed: null

  schema_check_type: ""
  drift_check_method: ""

  check_results:
    critical_checks:
      schema_is_match: null
      no_data_drift: null

    non_critical_checks:
      no_missing_values: null
      no_duplicate_rows: null

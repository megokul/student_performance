--- CODE DUMP | PART 3 of 4 ---


================================================================================
# PY FILE: src\student_performance\dbhandler\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\dbhandler\base_handler.py
================================================================================

from abc import ABC, abstractmethod
from pathlib import Path
import pandas as pd

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class DBHandler(ABC):
    """
    Abstract base class for all database/storage handlers.
    Enables unified behavior across PostgreSQL, MongoDB, CSV, etc.
    """

    def __enter__(self) -> "DBHandler":
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        try:
            self.close()
        except Exception as e:
            msg = "Error closing DBHandler."
            raise StudentPerformanceError(e, msg) from e

    @abstractmethod
    def close(self) -> None:
        pass

    @abstractmethod
    def load_from_source(self) -> pd.DataFrame:
        pass

    def load_from_csv(self, source: Path) -> pd.DataFrame:
        try:
            df = pd.read_csv(source)
            logger.info(f"DataFrame loaded from CSV: {source}")
            return df
        except Exception as e:
            msg = f"Failed to load DataFrame from CSV: '{source}'"
            raise StudentPerformanceError(e, msg) from e

================================================================================
# PY FILE: src\student_performance\dbhandler\postgres_dbhandler.py
================================================================================

import psycopg2
from psycopg2 import sql
import pandas as pd

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger
from src.student_performance.dbhandler.base_handler import DBHandler
from src.student_performance.entity.config_entity import PostgresDBHandlerConfig
from box import ConfigBox


class PostgresDBHandler(DBHandler):
    def __init__(self, postgres_config: PostgresDBHandlerConfig) -> None:
        logger.info("Initializing PostgresDBHandler")
        self.postgres_config = postgres_config
        self._connection: psycopg2.extensions.connection | None = None
        self._cursor: psycopg2.extensions.cursor | None = None

    def _connect(self) -> None:
        logger.info("Attempting to connect to PostgreSQL")
        if not self._connection or self._connection.closed:
            try:
                self._connection = psycopg2.connect(
                    host=self.postgres_config.host,
                    port=self.postgres_config.port,
                    dbname=self.postgres_config.dbname,
                    user=self.postgres_config.user,
                    password=self.postgres_config.password,
                )
                self._cursor = self._connection.cursor()
                logger.info("Successfully connected to PostgreSQL")
            except Exception as e:
                msg = "Failed to establish PostgreSQL connection"
                raise StudentPerformanceError(e, msg) from e

    def close(self) -> None:
        if self._cursor:
            self._cursor.close()
        if self._connection:
            self._connection.close()
            logger.info("PostgreSQL connection closed")

    def ping(self) -> None:
        logger.info("Pinging PostgreSQL")
        try:
            self._connect()
            logger.info("Executing ping query")
            self._cursor.execute("SELECT 1;")
            self._cursor.fetchone()
            logger.info("PostgreSQL connection successful (ping passed).")
        except Exception as e:
            msg = "PostgreSQL ping failed"
            raise StudentPerformanceError(e, msg) from e
        logger.info("PostgreSQL ping completed")

    def load_from_source(self) -> pd.DataFrame:
        logger.info(f"Loading data from PostgreSQL table: {self.postgres_config.table_name}")
        try:
            self._connect()
            query = sql.SQL("SELECT * FROM {}").format(sql.Identifier(self.postgres_config.table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            df = pd.read_sql_query(query.as_string(self._connection), self._connection)
            logger.info(f"DataFrame loaded from PostgreSQL table: {self.postgres_config.table_name}")
            return df
        except Exception as e:
            msg = f"Failed to load data from PostgreSQL table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e

    def get_table_list(self) -> list[str]:
        """
        Get a list of all tables in the PostgreSQL database.

        Returns:
            list[str]: A list of table names.

        Raises:
            StudentPerformanceError: If listing tables fails.
        """
        logger.info("Retrieving list of tables")
        try:
            self._connect()
            query = sql.SQL("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = 'public'
                AND table_type = 'BASE TABLE';
            """)
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            self._cursor.execute(query)
            tables = [table[0] for table in self._cursor.fetchall()]
            logger.info("Successfully retrieved list of tables.")
            return tables
        except Exception as e:
            msg = "Failed to retrieve list of tables"
            raise StudentPerformanceError(e, msg) from e

    def create_table_from_schema(self) -> None:
        """
        Create a PostgreSQL table if it doesn't exist using schema from ConfigBox.

        Args:
            table_name (str): The name of the table to create.
            schema (ConfigBox): Parsed schema.yaml with dot-access support.

        Raises:
            StudentPerformanceError: If table creation fails.
        """
        logger.info(f"Creating table from schema: {self.postgres_config.table_name}")
        try:
            self._connect()

            table_name = self.postgres_config.table_name

            # Check if table exists
            query = sql.SQL("""
                SELECT EXISTS (
                    SELECT 1
                    FROM information_schema.tables
                    WHERE table_name = {}
                );
            """).format(sql.Literal(table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            self._cursor.execute(query)
            table_exists = self._cursor.fetchone()[0]

            if table_exists:
                logger.info(f"Table '{table_name}' already exists.")
                return

            # Access columns via dot-notation
            table_schema = self.postgres_config.table_schema[table_name].columns

            column_definitions = []

            for col_name, col_def in table_schema.items():
                col_type = col_def.type
                constraints = col_def.get("constraints", {})

                column_sql = f"{col_name} {col_type}"

                # ENUM-style value check
                if "allowed_values" in constraints:
                    allowed = ", ".join("'{}'".format(val.replace("'", "''")) for val in constraints.allowed_values)
                    column_sql += f" CHECK ({col_name} IN ({allowed}))"

                # Numeric bounds
                if "min" in constraints and "max" in constraints:
                    column_sql += f" CHECK ({col_name} BETWEEN {constraints.min} AND {constraints.max})"
                elif "min" in constraints:
                    column_sql += f" CHECK ({col_name} >= {constraints.min})"
                elif "max" in constraints:
                    column_sql += f" CHECK ({col_name} <= {constraints.max})"

                column_definitions.append(column_sql)

            # Final CREATE query
            create_query = sql.SQL("""
                CREATE TABLE IF NOT EXISTS {} (
                    {}
                );
            """).format(
                sql.Identifier(table_name),
                sql.SQL(", ").join(map(sql.SQL, column_definitions))
            )
            logger.info(f"Executing query: {create_query.as_string(self._connection)}")
            self._cursor.execute(create_query)
            self._connection.commit()
            logger.info(f"Table '{table_name}' created.")

        except Exception as e:
            msg = f"Failed to create table: '{table_name}'"
            raise StudentPerformanceError(e, msg) from e
        logger.info(f"Finished creating table from schema: {self.postgres_config.table_name}")

    def insert_data_from_csv(self) -> None:
        """
        Insert data from the configured CSV file into the PostgreSQL table.

        Raises:
            StudentPerformanceError: If data insertion fails.
        """
        logger.info(f"Inserting data from CSV into table: {self.postgres_config.table_name}")
        try:
            self._connect()
            
            # Read the CSV file into a Pandas DataFrame
            csv_filepath = self.postgres_config.input_data_filepath
            logger.info(f"Reading CSV file: {csv_filepath}")
            df = pd.read_csv(csv_filepath)
            
            # Get the table name
            table_name = self.postgres_config.table_name
            
            # Define the SQL INSERT query
            columns = ', '.join(df.columns)
            values = ', '.join(['%s'] * len(df.columns))
            insert_query = sql.SQL("INSERT INTO {} ({}) VALUES ({})").format(
                sql.Identifier(table_name),
                sql.SQL(columns),
                sql.SQL(values)
            )
            
            # Execute the INSERT query for each row in the DataFrame
            logger.info("Inserting data into table")
            for _, row in df.iterrows():
                self._cursor.execute(insert_query, row.tolist())
            
            # Commit the changes to the database
            self._connection.commit()
            
        except Exception as e:
            msg = f"Failed to insert data from CSV into table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e
        logger.info(f"Successfully inserted data from CSV into table: {self.postgres_config.table_name}")
        logger.info(f"Finished inserting data from CSV into table: {self.postgres_config.table_name}")

    def read_data_to_df(self) -> pd.DataFrame:
        """
        Reads data from the PostgreSQL table into a Pandas DataFrame.

        Returns:
            pd.DataFrame: A Pandas DataFrame containing the data from the table.

        Raises:
            StudentPerformanceError: If reading data fails.
        """
        logger.info(f"Reading data from PostgreSQL table: {self.postgres_config.table_name}")
        try:
            self._connect()
            query = sql.SQL("SELECT * FROM {}").format(sql.Identifier(self.postgres_config.table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            df = pd.read_sql_query(query.as_string(self._connection), self._connection)
            logger.info(f"Successfully read data from PostgreSQL table: {self.postgres_config.table_name}")
            logger.info(f"Finished reading data from PostgreSQL table: {self.postgres_config.table_name}")
            return df
        except Exception as e:
            msg = f"Failed to read data from PostgreSQL table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e

================================================================================
# PY FILE: src\student_performance\dbhandler\s3_handler.py
================================================================================

from pathlib import Path
import os
import boto3
from botocore.exceptions import ClientError

import pandas as pd

from src.student_performance.entity.config_entity import S3HandlerConfig
from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger
from src.student_performance.dbhandler.base_handler import DBHandler


class S3Handler(DBHandler):
    """
    AWS S3 Handler for file and directory uploads.
    Implements DBHandler interface (for consistency).
    """

    def __init__(self, config: S3HandlerConfig) -> None:
        try:
            self.config = config
            self._client = boto3.client("s3", region_name=self.config.aws_region)
            logger.info(
                "S3Handler initialized for bucket '%s' in region '%s'",
                self.config.bucket_name,
                self.config.aws_region,
            )
        except Exception as e:
            logger.exception("Failed to initialize S3 client.")
            raise StudentPerformanceError(e, logger) from e

    def close(self) -> None:
        """
        No persistent connection to close. Present for DBHandler compatibility.
        """
        logger.info("S3Handler.close() called. No persistent connection to close.")

    def load_from_source(self) -> pd.DataFrame:
        """
        Not implemented: S3Handler does not support loading as DataFrame.
        """
        raise NotImplementedError("S3Handler does not support loading as DataFrame.")

    def upload_file(self, local_path: Path, s3_key: str) -> None:
        """
        Upload a single file to S3.

        Args:
            local_path (Path): Local file path.
            s3_key (str): S3 object key (destination path).
        """
        try:
            local_path = Path(local_path)
            if not local_path.is_file():
                raise FileNotFoundError(f"Local file not found: {local_path.as_posix()}")

            self._client.upload_file(
                Filename=str(local_path),
                Bucket=self.config.bucket_name,
                Key=s3_key
            )
            logger.info(
                "Uploaded: %s -> s3://%s/%s",
                local_path.as_posix(),
                self.config.bucket_name,
                s3_key, 
            )

        except ClientError as e:
            logger.error("AWS ClientError during file upload: %s", str(e))
            raise StudentPerformanceError(e, logger) from e
        except Exception as e:
            logger.error("Unexpected error during file upload: %s", str(e))
            raise StudentPerformanceError(e, logger) from e

    def sync_directory(self, local_dir: Path, s3_prefix: str) -> None:
        """
        Recursively uploads a directory to S3.

        Args:
            local_dir (Path): Local directory path.
            s3_prefix (str): S3 prefix (destination folder).
        """
        try:
            local_dir = Path(local_dir)
            if not local_dir.is_dir():
                raise NotADirectoryError(f"Local directory not found: {local_dir.as_posix()}")

            logger.info(
                "Starting directory sync: %s -> s3://%s/%s",
                local_dir.as_posix(),
                self.config.bucket_name,
                s3_prefix,
            )

            for root, _, files in os.walk(local_dir):
                for file in files:
                    local_file_path = Path(root) / file
                    relative_path = local_file_path.relative_to(local_dir)
                    remote_key = (Path(s3_prefix) / relative_path).as_posix()
                    self.upload_file(local_file_path, remote_key)

            logger.info(
                "Directory successfully synced: %s -> s3://%s/%s",
                local_dir.as_posix(),
                self.config.bucket_name,
                s3_prefix,
            )

        except Exception as e:
            logger.error("Directory sync to S3 failed.")
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\entity\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\entity\artifact_entity.py
================================================================================

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class DataIngestionArtifact:
    raw_artifact_path: Path
    ingested_data_filepath: Path
    dvc_raw_filepath: Path

    def __repr__(self) -> str:
        raw_artifact_str = self.raw_artifact_path.as_posix() if self.raw_artifact_path else "None"
        raw_dvc_str = self.dvc_raw_filepath.as_posix() if self.dvc_raw_filepath else "None"
        ingested_data_str = self.ingested_data_filepath.as_posix() if self.ingested_data_filepath else "None"

        return (
            "\nData Ingestion Artifact:\n"
            f"  - Raw Artifact:         '{raw_artifact_str}'\n"
            f"  - Raw DVC Path:         '{raw_dvc_str}'\n"
            f"  - Ingested Data Path:   '{ingested_data_str}'\n"
        )

@dataclass(frozen=True)
class DataValidationArtifact:
    validated_filepath: Path
    validation_status: bool

    def __repr__(self) -> str:
        validated_str = self.validated_filepath.as_posix() if self.validated_filepath else "None"

        return (
            "\nData Validation Artifact:\n"
            f"  - Validated Data Path: '{validated_str}'\n"
            f"  - Validation Status:   '{self.validation_status}'\n"
        )


@dataclass(frozen=True)
class DataTransformationArtifact:
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __repr__(self) -> str:
        return (
            "\nData Transformation Artifact:\n"
            f"  - X Train Path:         '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train Path:         '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val Path:           '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val Path:           '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test Path:          '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test Path:          '{self.y_test_filepath.as_posix()}'\n"
            f"  - X Preprocessor Path:  '{self.x_preprocessor_filepath.as_posix()}'\n"
            f"  - Y Preprocessor Path:  '{self.y_preprocessor_filepath.as_posix()}'\n"
        )

@dataclass(frozen=True)
class ModelTrainerArtifact:
    trained_model_filepath: Path
    training_report_filepath: Path
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    def __repr__(self) -> str:
        return (
            "\nModel Trainer Artifact:\n"
            f"  - Trained Model Path:   '{self.trained_model_filepath.as_posix()}'\n"
            f"  - Training Report Path: '{self.training_report_filepath.as_posix()}'\n"
            f"  - X Train Path: '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train Path: '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val Path:   '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val Path:   '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test Path:  '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test Path:  '{self.y_test_filepath.as_posix()}'"
        )

================================================================================
# PY FILE: src\student_performance\entity\config_entity.py
================================================================================

from box import ConfigBox
from dataclasses import dataclass
from pathlib import Path
from typing import List

@dataclass
class PostgresDBHandlerConfig:
    root_dir: Path
    host: str
    port: int
    dbname: str
    user: str
    password: str
    table_name: str
    input_data_filepath: Path
    table_schema: ConfigBox

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.input_data_filepath = Path(self.input_data_filepath)

    def __repr__(self) -> str:
        return (
            "\nPostgres DB Handler Config:\n"
            f"  - Root Dir:         '{self.root_dir.as_posix()}'\n"
            f"  - Host:             {self.host}\n"
            f"  - Port:             {self.port}\n"
            f"  - Database Name:    {self.dbname}\n"
            f"  - User:             {self.user}\n"
            f"  - Password:         {'*' * 8} (hidden)\n"
            f"  - Table:            {self.table_name}\n"
            f"  - Input Filepath:   '{self.input_data_filepath.as_posix()}'\n"
            f"  - Input Filepath:   {'table_schema'} (hidden)\n"
        )


@dataclass
class DataIngestionConfig:
    root_dir: Path
    raw_data_filepath: Path
    dvc_raw_filepath: Path
    ingested_data_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.raw_data_filepath = Path(self.raw_data_filepath)
        self.dvc_raw_filepath = Path(self.dvc_raw_filepath)
        self.ingested_data_filepath = Path(self.ingested_data_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Ingestion Config:\n"
            f"  - Root Dir:           '{self.root_dir.as_posix()}'\n"
            f"  - Raw Data Path:      '{self.raw_data_filepath.as_posix()}'\n"
            f"  - DVC Raw Path:       '{self.dvc_raw_filepath.as_posix()}'\n"
            f"  - Ingested Data Path: '{self.ingested_data_filepath.as_posix()}'\n"
        )


@dataclass
class DataValidationConfig:
    root_dir: Path
    validated_filepath: Path
    dvc_validated_filepath: Path

    schema: ConfigBox
    report_template: ConfigBox
    validation_params: ConfigBox

    missing_report_filepath: Path
    duplicates_report_filepath: Path
    drift_report_filepath: Path
    validation_report_filepath: Path
    categorical_report_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.validated_filepath = Path(self.validated_filepath)
        self.dvc_validated_filepath = Path(self.dvc_validated_filepath)

        self.missing_report_filepath = Path(self.missing_report_filepath)
        self.duplicates_report_filepath = Path(self.duplicates_report_filepath)
        self.drift_report_filepath = Path(self.drift_report_filepath)
        self.validation_report_filepath = Path(self.validation_report_filepath)
        self.categorical_report_filepath = Path(self.categorical_report_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Validation Config:\n"
            f"  - Root Dir:                '{self.root_dir.as_posix()}'\n"
            f"  - Validated CSV:           '{self.validated_filepath.as_posix()}'\n"
            f"  - DVC Validated Path:      '{self.dvc_validated_filepath.as_posix()}'\n"
            f"  - Missing Report:          '{self.missing_report_filepath.as_posix()}'\n"
            f"  - Duplicates Report:       '{self.duplicates_report_filepath.as_posix()}'\n"
            f"  - Drift Report:            '{self.drift_report_filepath.as_posix()}'\n"
            f"  - Categorical Report:      '{self.categorical_report_filepath.as_posix()}'\n"
            f"  - Validation Report:       '{self.validation_report_filepath.as_posix()}'\n"
            f"  - Schema Config:           'schema' (hidden)\n"
            f"  - Report Template:         'template' (hidden)\n"
            f"  - Validation Params:       'params' (hidden)\n"
        )


@dataclass
class DataTransformationConfig:
    root_dir: Path

    # Target and parameters
    target_column: str
    transformation_params: ConfigBox

    # Transformed dataset filepaths
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    # DVC-tracked filepaths
    x_train_dvc_filepath: Path
    y_train_dvc_filepath: Path
    x_val_dvc_filepath: Path
    y_val_dvc_filepath: Path
    x_test_dvc_filepath: Path
    y_test_dvc_filepath: Path

    # Preprocessor objects
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.x_train_filepath = Path(self.x_train_filepath)
        self.y_train_filepath = Path(self.y_train_filepath)
        self.x_val_filepath = Path(self.x_val_filepath)
        self.y_val_filepath = Path(self.y_val_filepath)
        self.x_test_filepath = Path(self.x_test_filepath)
        self.y_test_filepath = Path(self.y_test_filepath)

        self.x_train_dvc_filepath = Path(self.x_train_dvc_filepath)
        self.y_train_dvc_filepath = Path(self.y_train_dvc_filepath)
        self.x_val_dvc_filepath = Path(self.x_val_dvc_filepath)
        self.y_val_dvc_filepath = Path(self.y_val_dvc_filepath)
        self.x_test_dvc_filepath = Path(self.x_test_dvc_filepath)
        self.y_test_dvc_filepath = Path(self.y_test_dvc_filepath)

        self.x_preprocessor_filepath = Path(self.x_preprocessor_filepath)
        self.y_preprocessor_filepath = Path(self.y_preprocessor_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Transformation Config:\n"
            f"  - Root Dir:              '{self.root_dir.as_posix()}'\n"
            f"  - Target Column:         '{self.target_column}'\n"
            f"  - X Train:               '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train:               '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val:                 '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val:                 '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test:                '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test:                '{self.y_test_filepath.as_posix()}'\n"
            f"  - X Preprocessor:        '{self.x_preprocessor_filepath.as_posix()}'\n"
            f"  - Y Preprocessor:        '{self.y_preprocessor_filepath.as_posix()}'\n"
            f"  - Transformation Params: 'transformation_params' (hidden)\n"
        )

@dataclass
class ModelTrainerConfig:
    root_dir: Path
    models: List[dict]
    optimization: ConfigBox
    tracking: ConfigBox
    trained_model_filepath: Path
    training_report_filepath: Path

    def __post_init__(self):
        self.trained_model_filepath = Path(self.trained_model_filepath)
        self.training_report_filepath = Path(self.trained_model_filepath)

    def __repr__(self) -> str:
        return (
            "\nModel Trainer Config:\n"
            f"  - Root Dir:               '{self.root_dir.as_posix()}'\n"
            f"  - Trained Model:          '{self.trained_model_filepath.as_posix()}'\n"
            f"  - Training Report:        '{self.training_report_filepath.as_posix()}'\n"
            f"  - Models:                 'models' (hidden)\n"
            f"  - Optimization:           'optimization' (hidden)\n"
            f"  - Tracking:               'tracking' (hidden)"
        )

================================================================================
# PY FILE: src\student_performance\exception\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\exception\exception.py
================================================================================

import sys
from types import TracebackType
from logging import Logger


class StudentPerformanceError(Exception):
    """
    Custom exception for the Student Performance project.

    Automatically captures:
    - Original exception message
    - Filename and line number from the traceback
    - Logs the formatted error using an injected logger
    """

    def __init__(self, error: Exception, logger: Logger) -> None:
        # Set the core message
        super().__init__(str(error))
        self.message: str = str(error)
        self.logger: Logger = logger

        # Extract traceback info from current exception context
        _, _, tb = sys.exc_info()
        tb: TracebackType | None

        # Safely capture line number and file
        self.line: int | None = tb.tb_lineno if tb and tb.tb_lineno else None
        self.file: str = tb.tb_frame.f_code.co_filename if tb and tb.tb_frame else "Unknown"

        # Log the error using injected logger with traceback
        try:
            self.logger.error(str(self), exc_info=True)
        except Exception as log_error:
            print(f"Logging failed inside StudentPerformanceError: {log_error}")

    def __str__(self) -> str:
        return (
            f"Error occurred in file [{self.file}], "
            f"line [{self.line}], "
            f"message: [{self.message}]"
        )

================================================================================
# PY FILE: src\student_performance\logging\__init__.py
================================================================================

"""
Centralized logger for the student_performance project.

Provides a reusable `logger` instance configured with:
- UTC timestamped log directory and file
- File + stream handlers
- DEBUG level logging by default
"""

import logging
from .app_logger import setup_logger

# Static logger name and level
LOGGER_NAME = "student_performance_logger"
LOG_LEVEL = logging.DEBUG

# Initialize logger once and share across the project
logger = setup_logger(name=LOGGER_NAME, level=LOG_LEVEL)

================================================================================
# PY FILE: src\student_performance\logging\app_logger.py
================================================================================

import logging
import sys
from pathlib import Path
from src.student_performance.constants.constants import LOGS_ROOT
from src.student_performance.utils.timestamp import get_utc_timestamp

def setup_logger(name: str = "app_logger", level: int = logging.DEBUG) -> logging.Logger:
    """
    Set up and return a logger with file and stream handlers, allowing custom log level.

    Args:
        name (str): Name of the logger.
        level (int): Logging level (e.g., logging.DEBUG, logging.INFO).

    Returns:
        logging.Logger: Configured logger instance.
    """
    sys.stdout.reconfigure(encoding="utf-8")
    timestamp = get_utc_timestamp()

    log_dir = Path(LOGS_ROOT) / timestamp
    log_dir.mkdir(parents=True, exist_ok=True)
    log_filepath = log_dir / f"{timestamp}.log"

    log_format = "[%(asctime)s] - %(levelname)s - %(module)s - %(message)s"
    formatter = logging.Formatter(log_format)

    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Add file handler if not already added
    if not any(isinstance(h, logging.FileHandler) and h.baseFilename == str(log_filepath)
               for h in logger.handlers):
        file_handler = logging.FileHandler(log_filepath, mode="a")
        file_handler.setFormatter(formatter)
        file_handler.setLevel(level)
        logger.addHandler(file_handler)

    # Add stdout stream handler if not already added
    if not any(isinstance(h, logging.StreamHandler) and h.stream == sys.stdout
               for h in logger.handlers):
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setFormatter(formatter)
        stream_handler.setLevel(level)
        logger.addHandler(stream_handler)

    return logger

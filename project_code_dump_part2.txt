
ðŸ“¦ Project Structure of: e:\MyProjects\student_performance

ðŸ“„ .env
ðŸ“„ .gitignore
ðŸ“„ Dockerfile
ðŸ“„ app.py
ðŸ“ artifacts/
    ðŸ“ 2025_06_08T02_47_45Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_08T02_58_11Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_11T14_20_11Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_11T14_21_21Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_11T14_24_09Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_11T14_24_38Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
    ðŸ“ 2025_06_11T14_26_14Z/
        ðŸ“ data_ingestion/
            ðŸ“ ingested_data/
                ðŸ“„ ingested_data.csv
            ðŸ“ raw_data/
                ðŸ“„ raw.csv
        ðŸ“ data_validation/
            ðŸ“ reports/
                ðŸ“„ categorical_report.yaml
                ðŸ“„ drift_report.yaml
                ðŸ“„ duplicates_report.yaml
                ðŸ“„ missing_values_report.yaml
                ðŸ“„ validation_report.yaml
            ðŸ“ validated/
                ðŸ“„ validated_data.csv
ðŸ“ config/
    ðŸ“„ config.yaml
    ðŸ“„ params.yaml
    ðŸ“„ schema.yaml
    ðŸ“„ templates.yaml
ðŸ“ data/
    ðŸ“ raw/
        ðŸ“„ raw.csv
    ðŸ“ validated/
        ðŸ“„ validated_data.csv
ðŸ“„ debug.py
ðŸ“ logs/
    ðŸ“ 2025_06_08T02_47_45Z/
        ðŸ“„ 2025_06_08T02_47_45Z.log
    ðŸ“ 2025_06_08T02_58_11Z/
        ðŸ“„ 2025_06_08T02_58_11Z.log
    ðŸ“ 2025_06_11T14_09_30Z/
        ðŸ“„ 2025_06_11T14_09_30Z.log
    ðŸ“ 2025_06_11T14_11_03Z/
        ðŸ“„ 2025_06_11T14_11_03Z.log
    ðŸ“ 2025_06_11T14_20_11Z/
        ðŸ“„ 2025_06_11T14_20_11Z.log
    ðŸ“ 2025_06_11T14_21_21Z/
        ðŸ“„ 2025_06_11T14_21_21Z.log
    ðŸ“ 2025_06_11T14_24_09Z/
        ðŸ“„ 2025_06_11T14_24_09Z.log
    ðŸ“ 2025_06_11T14_24_38Z/
        ðŸ“„ 2025_06_11T14_24_38Z.log
    ðŸ“ 2025_06_11T14_26_14Z/
        ðŸ“„ 2025_06_11T14_26_14Z.log
    ðŸ“ 2025_06_11T15_45_36Z/
        ðŸ“„ 2025_06_11T15_45_36Z.log
    ðŸ“ 2025_06_11T16_21_13Z/
        ðŸ“„ 2025_06_11T16_21_13Z.log
ðŸ“„ main.py
ðŸ“ notebook/
    ðŸ“„ research.ipynb
ðŸ“„ project_code_dump_index.txt
ðŸ“„ project_code_dump_part1.txt
ðŸ“„ project_code_dump_part2.txt
ðŸ“„ project_dump.py
ðŸ“„ project_template.py
ðŸ“„ requirements.txt
ðŸ“ research/
    ðŸ“„ research.ipynb
ðŸ“„ setup.py
ðŸ“ src/
    ðŸ“ student_performance/
        ðŸ“„ __init__.py
        ðŸ“ components/
            ðŸ“„ __init__.py
            ðŸ“„ data_ingestion.py
            ðŸ“„ data_transformation.py
            ðŸ“„ data_validation.py
        ðŸ“ config/
            ðŸ“„ __init__.py
            ðŸ“„ configuration.py
        ðŸ“ constants/
            ðŸ“„ __init__.py
            ðŸ“„ constants.py
        ðŸ“ data_processors/
            ðŸ“„ __init__.py
            ðŸ“„ column_math_factory.py
            ðŸ“„ encoder_factory.py
            ðŸ“„ imputer_factory.py
            ðŸ“„ preprocessor_builder.py
            ðŸ“„ scaler_factory.py
        ðŸ“ dbhandler/
            ðŸ“„ __init__.py
            ðŸ“„ base_handler.py
            ðŸ“„ postgres_dbhandler.py
        ðŸ“ entity/
            ðŸ“„ __init__.py
            ðŸ“„ artifact_entity.py
            ðŸ“„ config_entity.py
        ðŸ“ exception/
            ðŸ“„ __init__.py
            ðŸ“„ exception.py
        ðŸ“ logging/
            ðŸ“„ __init__.py
            ðŸ“„ app_logger.py
        ðŸ“ pipeline/
            ðŸ“„ __init__.py
            ðŸ“„ training_pipeline.py
        ðŸ“ utils/
            ðŸ“„ __init__.py
            ðŸ“„ core.py
            ðŸ“„ timestamp.py
ðŸ“ student_data/
    ðŸ“„ stud.csv
ðŸ“ templates/
    ðŸ“„ index.html

--- CODE DUMP | PART 2 of 3 ---


================================================================================
# PY FILE: src\student_performance\constants\constants.py
================================================================================

CONFIG_ROOT = "config"
CONFIG_FILENAME = "config.yaml"
PARAMS_FILENAME = "params.yaml"
SCHEMA_FILENAME = "schema.yaml"
TEMPLATES_FILENAME = "templates.yaml"

LOGS_ROOT = "logs"

ARTIFACTS_ROOT = "artifacts"

POSTGRES_HANDLER_ROOT = "mongo_handler"
INGEST_ROOT = "data_ingestion"
INGEST_RAW_SUBDIR = "raw_data"
INGEST_INGESTED_SUBDIR = "ingested_data"

DVC_ROOT = "data"
DVC_RAW_SUBDIR = "raw"
DVC_VALIDATED_SUBDIR = "validated"
DVC_TRANSFORMED_SUBDIR = "transformed"

VALID_ROOT = "data_validation"
VALID_VALIDATED_SUBDIR = "validated"
VALID_REPORTS_SUBDIR = "reports"

X_TRAIN_LABEL = "X_train"
Y_TRAIN_LABEL = "y_train"
X_VAL_LABEL = "X_val"
Y_VAL_LABEL = "y_val"
X_TEST_LABEL = "X_test"
Y_TEST_LABEL = "y_test"

================================================================================
# PY FILE: src\student_performance\data_processors\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\data_processors\column_math_factory.py
================================================================================

from typing import Literal, Self
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from ensure import ensure_annotations

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger

# Define allowed operations
ColumnMathOperation = Literal[
    "add", "subtract", "multiply", "divide",
    "mean", "sqrt", "square", "power"
]


class ColumnMathTransformer(BaseEstimator, TransformerMixin):
    """
    Transformer that performs mathematical operations on specified columns.

    Supported operations:
        - add: sum of multiple columns
        - subtract: subtract columns sequentially (col1 - col2 - col3 ...)
        - multiply: product of multiple columns
        - divide: divide columns sequentially (col1 / col2 / col3 ...)
        - mean: average of specified columns
        - sqrt: element-wise square root of first column
        - square: element-wise square of first column
        - power: first column raised to power of second

    Raises:
        StudentPerformanceError: For unsupported operations or missing params
    """

    @ensure_annotations
    def __init__(
        self,
        columns: list[str],
        operation: ColumnMathOperation,
        output_column: str
    ) -> None:
        self.columns = columns
        self.operation = operation.lower()
        self.output_column = output_column

    @ensure_annotations
    def fit(self, X: pd.DataFrame, y: pd.Series | None = None) -> Self:
        return self

    @ensure_annotations
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        try:
            df = X.copy()

            if self.operation == "add":
                df[self.output_column] = df[self.columns].sum(axis=1)

            elif self.operation == "subtract":
                df[self.output_column] = df[self.columns].iloc[:, 0]
                for col in self.columns[1:]:
                    df[self.output_column] -= df[col]

            elif self.operation == "multiply":
                df[self.output_column] = df[self.columns].prod(axis=1)

            elif self.operation == "divide":
                df[self.output_column] = df[self.columns].iloc[:, 0]
                for col in self.columns[1:]:
                    df[self.output_column] /= df[col]

            elif self.operation == "mean":
                df[self.output_column] = df[self.columns].mean(axis=1)

            elif self.operation == "sqrt":
                if len(self.columns) != 1:
                    raise ValueError("sqrt operation requires exactly 1 column.")
                df[self.output_column] = np.sqrt(df[self.columns[0]])

            elif self.operation == "square":
                if len(self.columns) != 1:
                    raise ValueError("square operation requires exactly 1 column.")
                df[self.output_column] = df[self.columns[0]] ** 2

            elif self.operation == "power":
                if len(self.columns) != 2:
                    raise ValueError("power operation requires exactly 2 columns.")
                df[self.output_column] = df[self.columns[0]] ** df[self.columns[1]]

            else:
                raise ValueError(f"Unsupported operation: {self.operation}")

            logger.info(
                "Applied '%s' operation on columns: %s â†’ '%s'",
                self.operation,
                self.columns,
                self.output_column,
            )
            return df

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\data_processors\encoder_factory.py
================================================================================

"""Factory for building encoding pipelines."""

from typing import Optional

from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class EncoderFactory:
    """
    Factory to build encoding pipelines for categorical features.
    Supports: onehot, ordinal.
    Easily extendable with new encoders.
    """
    _SUPPORTED_METHODS = {
        "onehot": OneHotEncoder,
        "ordinal": OrdinalEncoder,
    }

    @staticmethod
    @ensure_annotations
    def get_encoder_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False,
    ) -> Pipeline:
        """
        Returns a sklearn Pipeline with a single encoder step.

        Args:
            method: Encoding method name.
            params: Parameters for the encoder constructor.
            is_target: Whether this encoder is for the target variable.
                       Not used by default but available for customization.

        Returns:
            Pipeline: A pipeline containing one ('encoder', encoder_instance) step.
        """
        try:
            if method not in EncoderFactory._SUPPORTED_METHODS:
                raise ValueError(f"Unsupported encoding method: {method}")

            encoder_class = EncoderFactory._SUPPORTED_METHODS[method]
            encoder = encoder_class(**(params or {}))

            return Pipeline(steps=[("encoder", encoder)])

        except Exception as exc:
            raise StudentPerformanceError(exc, logger) from exc

================================================================================
# PY FILE: src\student_performance\data_processors\imputer_factory.py
================================================================================

"""Factory for building encoding pipelines."""

from typing import Optional

from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class ImputerFactory:
    """
    Factory to build imputation pipelines for numerical data.
    Supports: knn, simple, iterative, and custom methods.
    Easily extendable with new methods.
    """
    _SUPPORTED_METHODS = {
        "knn": KNNImputer,
        "simple": SimpleImputer,
        "iterative": IterativeImputer,
    }

    @staticmethod
    @ensure_annotations
    def get_imputer_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False,
    ) -> Pipeline:
        """
        Returns a sklearn Pipeline with a single imputer step.

        Args:
            method: Imputation method name.
            params: Parameters for the imputer constructor.
            is_target: Whether this imputer is for the target variable.
                       Not used by default but available for customization.

        Returns:
            Pipeline: A pipeline containing one ('imputer', imputer_instance) step.
        """
        try:
            if method == "custom":
                if not params or "custom_callable" not in params:
                    raise ValueError(
                        "Custom imputer requires a 'custom_callable' in params."
                    )
                imputer = params["custom_callable"]()
            else:
                ImputerClass = ImputerFactory._SUPPORTED_METHODS.get(method)
                if not ImputerClass:
                    raise ValueError(f"Unsupported imputation method: {method}")
                imputer = ImputerClass(**(params or {}))

            return Pipeline(steps=[("imputer", imputer)])

        except Exception as exc:
            raise StudentPerformanceError(exc, logger) from exc

================================================================================
# PY FILE: src\student_performance\data_processors\preprocessor_builder.py
================================================================================

# FILE: src/student_performance/data_processors/preprocessor_builder.py

from typing import Tuple
from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.experimental import enable_iterative_imputer  # noqa: F401

from src.student_performance.data_processors.imputer_factory import (
    ImputerFactory,
)
from src.student_performance.data_processors.scaler_factory import (
    ScalerFactory,
)
from src.student_performance.data_processors.encoder_factory import (
    EncoderFactory,
)
from student_performance.data_processors.column_math_factory import (
    MeanOfColumnsTargetComputer,
)
from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class PreprocessorBuilder:
    """
    Builds preprocessing pipelines for features (X) and target (y) using
    ordered steps defined in params.yaml. Skips any steps set to 'none'.
    """

    STEP_BUILDERS = {
        "imputation": ImputerFactory.get_imputer_pipeline,
        "encoding": EncoderFactory.get_encoder_pipeline,
        "standardization": ScalerFactory.get_scaler_pipeline,
        "compute_target": (
            lambda method, params: MeanOfColumnsTargetComputer(
                source_columns=params.get("source_columns", []),
                output_column=params.get("output_column", "target"),
            )
        ),
    }

    @ensure_annotations
    def __init__(self, steps: dict, methods: dict) -> None:
        self.steps = steps or {}
        self.methods = methods or {}

    def _build_pipeline(self, section: str) -> Pipeline:
        try:
            pipeline_steps = []
            ordered_steps = self.steps.get(section, [])
            method_map = self.methods.get(section, {})

            for step_name in ordered_steps:
                params = method_map.get(step_name, {})
                method = params.get("method")

                # Skip disabled or undefined
                if not method or str(method).lower() == "none":
                    logger.info(
                        f"Skipping '{step_name}' for '{section}' because it is disabled."
                    )
                    continue

                builder = self.STEP_BUILDERS.get(step_name)
                if not builder:
                    raise ValueError(
                        f"Unsupported step '{step_name}' in '{section}' pipeline."
                    )

                component = builder(method, params)
                pipeline_steps.append((step_name, component))

            return Pipeline(steps=pipeline_steps)

        except Exception as exc:
            raise StudentPerformanceError(exc, logger) from exc

    @ensure_annotations
    def build(self) -> Tuple[Pipeline, Pipeline]:
        """
        Build and return a tuple of (x_pipeline, y_pipeline).
        """
        try:
            x_pipeline = self._build_pipeline("x")
            y_pipeline = self._build_pipeline("y")
            return x_pipeline, y_pipeline
        except Exception as exc:
            raise StudentPerformanceError(exc, logger) from exc

================================================================================
# PY FILE: src\student_performance\data_processors\scaler_factory.py
================================================================================

"""Factory for building scaling pipelines for numerical data."""

from typing import Optional

from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class ScalerFactory:
    """
    Factory to build scaling pipelines for numerical features.
    Supports: standard, minmax, robust.
    Easily extendable with new scalers.
    """
    _SUPPORTED_METHODS = {
        "standard": StandardScaler,
        "minmax": MinMaxScaler,
        "robust": RobustScaler,
    }

    @staticmethod
    @ensure_annotations
    def get_scaler_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False,
    ) -> Pipeline:
        """
        Returns a sklearn Pipeline with a single scaler step.

        Args:
            method: Scaling method name.
            params: Parameters for the scaler constructor.
            is_target: Whether this scaler is for the target variable (unused by default).

        Returns:
            Pipeline: Pipeline containing one ('scaler', scaler_instance).
        """
        try:
            if method not in ScalerFactory._SUPPORTED_METHODS:
                raise ValueError(f"Unsupported scaler method: {method}")

            scaler_cls = ScalerFactory._SUPPORTED_METHODS[method]
            scaler = scaler_cls(**(params or {}))

            return Pipeline(steps=[("scaler", scaler)])

        except Exception as exc:
            raise StudentPerformanceError(exc, logger) from exc

================================================================================
# PY FILE: src\student_performance\dbhandler\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\dbhandler\base_handler.py
================================================================================

from abc import ABC, abstractmethod
from pathlib import Path
import pandas as pd

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class DBHandler(ABC):
    """
    Abstract base class for all database/storage handlers.
    Enables unified behavior across PostgreSQL, MongoDB, CSV, etc.
    """

    def __enter__(self) -> "DBHandler":
        return self

    def __exit__(self, exc_type, exc_val, exc_tb) -> None:
        try:
            self.close()
        except Exception as e:
            msg = "Error closing DBHandler."
            raise StudentPerformanceError(e, msg) from e

    @abstractmethod
    def close(self) -> None:
        pass

    @abstractmethod
    def load_from_source(self) -> pd.DataFrame:
        pass

    def load_from_csv(self, source: Path) -> pd.DataFrame:
        try:
            df = pd.read_csv(source)
            logger.info(f"DataFrame loaded from CSV: {source}")
            return df
        except Exception as e:
            msg = f"Failed to load DataFrame from CSV: '{source}'"
            raise StudentPerformanceError(e, msg) from e

================================================================================
# PY FILE: src\student_performance\dbhandler\postgres_dbhandler.py
================================================================================

import psycopg2
from psycopg2 import sql
import pandas as pd

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger
from src.student_performance.dbhandler.base_handler import DBHandler
from src.student_performance.entity.config_entity import PostgresDBHandlerConfig
from box import ConfigBox


class PostgresDBHandler(DBHandler):
    def __init__(self, postgres_config: PostgresDBHandlerConfig) -> None:
        logger.info("Initializing PostgresDBHandler")
        self.postgres_config = postgres_config
        self._connection: psycopg2.extensions.connection | None = None
        self._cursor: psycopg2.extensions.cursor | None = None

    def _connect(self) -> None:
        logger.info("Attempting to connect to PostgreSQL")
        if not self._connection or self._connection.closed:
            try:
                self._connection = psycopg2.connect(
                    host=self.postgres_config.host,
                    port=self.postgres_config.port,
                    dbname=self.postgres_config.dbname,
                    user=self.postgres_config.user,
                    password=self.postgres_config.password,
                )
                self._cursor = self._connection.cursor()
                logger.info("Successfully connected to PostgreSQL")
            except Exception as e:
                msg = "Failed to establish PostgreSQL connection"
                raise StudentPerformanceError(e, msg) from e

    def close(self) -> None:
        if self._cursor:
            self._cursor.close()
        if self._connection:
            self._connection.close()
            logger.info("PostgreSQL connection closed")

    def ping(self) -> None:
        logger.info("Pinging PostgreSQL")
        try:
            self._connect()
            logger.info("Executing ping query")
            self._cursor.execute("SELECT 1;")
            self._cursor.fetchone()
            logger.info("PostgreSQL connection successful (ping passed).")
        except Exception as e:
            msg = "PostgreSQL ping failed"
            raise StudentPerformanceError(e, msg) from e
        logger.info("PostgreSQL ping completed")

    def load_from_source(self) -> pd.DataFrame:
        logger.info(f"Loading data from PostgreSQL table: {self.postgres_config.table_name}")
        try:
            self._connect()
            query = sql.SQL("SELECT * FROM {}").format(sql.Identifier(self.postgres_config.table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            df = pd.read_sql_query(query.as_string(self._connection), self._connection)
            logger.info(f"DataFrame loaded from PostgreSQL table: {self.postgres_config.table_name}")
            return df
        except Exception as e:
            msg = f"Failed to load data from PostgreSQL table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e

    def get_table_list(self) -> list[str]:
        """
        Get a list of all tables in the PostgreSQL database.

        Returns:
            list[str]: A list of table names.

        Raises:
            StudentPerformanceError: If listing tables fails.
        """
        logger.info("Retrieving list of tables")
        try:
            self._connect()
            query = sql.SQL("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = 'public'
                AND table_type = 'BASE TABLE';
            """)
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            self._cursor.execute(query)
            tables = [table[0] for table in self._cursor.fetchall()]
            logger.info("Successfully retrieved list of tables.")
            return tables
        except Exception as e:
            msg = "Failed to retrieve list of tables"
            raise StudentPerformanceError(e, msg) from e

    def create_table_from_schema(self) -> None:
        """
        Create a PostgreSQL table if it doesn't exist using schema from ConfigBox.

        Args:
            table_name (str): The name of the table to create.
            schema (ConfigBox): Parsed schema.yaml with dot-access support.

        Raises:
            StudentPerformanceError: If table creation fails.
        """
        logger.info(f"Creating table from schema: {self.postgres_config.table_name}")
        try:
            self._connect()

            table_name = self.postgres_config.table_name

            # Check if table exists
            query = sql.SQL("""
                SELECT EXISTS (
                    SELECT 1
                    FROM information_schema.tables
                    WHERE table_name = {}
                );
            """).format(sql.Literal(table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            self._cursor.execute(query)
            table_exists = self._cursor.fetchone()[0]

            if table_exists:
                logger.info(f"Table '{table_name}' already exists.")
                return

            # Access columns via dot-notation
            table_schema = self.postgres_config.table_schema[table_name].columns

            column_definitions = []

            for col_name, col_def in table_schema.items():
                col_type = col_def.type
                constraints = col_def.get("constraints", {})

                column_sql = f"{col_name} {col_type}"

                # ENUM-style value check
                if "allowed_values" in constraints:
                    allowed = ", ".join("'{}'".format(val.replace("'", "''")) for val in constraints.allowed_values)
                    column_sql += f" CHECK ({col_name} IN ({allowed}))"

                # Numeric bounds
                if "min" in constraints and "max" in constraints:
                    column_sql += f" CHECK ({col_name} BETWEEN {constraints.min} AND {constraints.max})"
                elif "min" in constraints:
                    column_sql += f" CHECK ({col_name} >= {constraints.min})"
                elif "max" in constraints:
                    column_sql += f" CHECK ({col_name} <= {constraints.max})"

                column_definitions.append(column_sql)

            # Final CREATE query
            create_query = sql.SQL("""
                CREATE TABLE IF NOT EXISTS {} (
                    {}
                );
            """).format(
                sql.Identifier(table_name),
                sql.SQL(", ").join(map(sql.SQL, column_definitions))
            )
            logger.info(f"Executing query: {create_query.as_string(self._connection)}")
            self._cursor.execute(create_query)
            self._connection.commit()
            logger.info(f"Table '{table_name}' created.")

        except Exception as e:
            msg = f"Failed to create table: '{table_name}'"
            raise StudentPerformanceError(e, msg) from e
        logger.info(f"Finished creating table from schema: {self.postgres_config.table_name}")

    def insert_data_from_csv(self) -> None:
        """
        Insert data from the configured CSV file into the PostgreSQL table.

        Raises:
            StudentPerformanceError: If data insertion fails.
        """
        logger.info(f"Inserting data from CSV into table: {self.postgres_config.table_name}")
        try:
            self._connect()
            
            # Read the CSV file into a Pandas DataFrame
            csv_filepath = self.postgres_config.input_data_filepath
            logger.info(f"Reading CSV file: {csv_filepath}")
            df = pd.read_csv(csv_filepath)
            
            # Get the table name
            table_name = self.postgres_config.table_name
            
            # Define the SQL INSERT query
            columns = ', '.join(df.columns)
            values = ', '.join(['%s'] * len(df.columns))
            insert_query = sql.SQL("INSERT INTO {} ({}) VALUES ({})").format(
                sql.Identifier(table_name),
                sql.SQL(columns),
                sql.SQL(values)
            )
            
            # Execute the INSERT query for each row in the DataFrame
            logger.info("Inserting data into table")
            for _, row in df.iterrows():
                self._cursor.execute(insert_query, row.tolist())
            
            # Commit the changes to the database
            self._connection.commit()
            
        except Exception as e:
            msg = f"Failed to insert data from CSV into table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e
        logger.info(f"Successfully inserted data from CSV into table: {self.postgres_config.table_name}")
        logger.info(f"Finished inserting data from CSV into table: {self.postgres_config.table_name}")

    def read_data_to_df(self) -> pd.DataFrame:
        """
        Reads data from the PostgreSQL table into a Pandas DataFrame.

        Returns:
            pd.DataFrame: A Pandas DataFrame containing the data from the table.

        Raises:
            StudentPerformanceError: If reading data fails.
        """
        logger.info(f"Reading data from PostgreSQL table: {self.postgres_config.table_name}")
        try:
            self._connect()
            query = sql.SQL("SELECT * FROM {}").format(sql.Identifier(self.postgres_config.table_name))
            logger.info(f"Executing query: {query.as_string(self._connection)}")
            df = pd.read_sql_query(query.as_string(self._connection), self._connection)
            logger.info(f"Successfully read data from PostgreSQL table: {self.postgres_config.table_name}")
            logger.info(f"Finished reading data from PostgreSQL table: {self.postgres_config.table_name}")
            return df
        except Exception as e:
            msg = f"Failed to read data from PostgreSQL table: {self.postgres_config.table_name}"
            raise StudentPerformanceError(e, msg) from e

================================================================================
# PY FILE: src\student_performance\entity\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\entity\artifact_entity.py
================================================================================

from box import ConfigBox
from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class DataIngestionArtifact:
    raw_artifact_path: Path
    ingested_data_filepath: Path
    dvc_raw_filepath: Path

    def __repr__(self) -> str:
        raw_artifact_str = self.raw_artifact_path.as_posix() if self.raw_artifact_path else "None"
        raw_dvc_str = self.dvc_raw_filepath.as_posix() if self.dvc_raw_filepath else "None"
        ingested_data_str = self.ingested_data_filepath.as_posix() if self.ingested_data_filepath else "None"

        return (
            "\nData Ingestion Artifact:\n"
            f"  - Raw Artifact:         '{raw_artifact_str}'\n"
            f"  - Raw DVC Path:         '{raw_dvc_str}'\n"
            f"  - Ingested Data Path:   '{ingested_data_str}'\n"
        )

@dataclass(frozen=True)
class DataValidationArtifact:
    validated_filepath: Path
    validation_status: bool

    def __repr__(self) -> str:
        validated_str = self.validated_filepath.as_posix() if self.validated_filepath else "None"

        return (
            "\nData Validation Artifact:\n"
            f"  - Validated Data Path: '{validated_str}'\n"
            f"  - Validation Status:   '{self.validation_status}'\n"
        )


@dataclass(frozen=True)
class DataTransformationArtifact:
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __repr__(self) -> str:
        return (
            "\nData Transformation Artifact:\n"
            f"  - X Train Path:         '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train Path:         '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val Path:           '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val Path:           '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test Path:          '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test Path:          '{self.y_test_filepath.as_posix()}'\n"
            f"  - X Preprocessor Path:  '{self.x_preprocessor_filepath.as_posix()}'\n"
            f"  - Y Preprocessor Path:  '{self.y_preprocessor_filepath.as_posix()}'\n"
        )

================================================================================
# PY FILE: src\student_performance\entity\config_entity.py
================================================================================

from box import ConfigBox
from dataclasses import dataclass
from pathlib import Path


@dataclass
class PostgresDBHandlerConfig:
    root_dir: Path
    host: str
    port: int
    dbname: str
    user: str
    password: str
    table_name: str
    input_data_filepath: Path
    table_schema: ConfigBox

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.input_data_filepath = Path(self.input_data_filepath)

    def __repr__(self) -> str:
        return (
            "\nPostgres DB Handler Config:\n"
            f"  - Root Dir:         '{self.root_dir.as_posix()}'\n"
            f"  - Host:             {self.host}\n"
            f"  - Port:             {self.port}\n"
            f"  - Database Name:    {self.dbname}\n"
            f"  - User:             {self.user}\n"
            f"  - Password:         {'*' * 8} (hidden)\n"
            f"  - Table:            {self.table_name}\n"
            f"  - Input Filepath:   '{self.input_data_filepath.as_posix()}'\n"
            f"  - Input Filepath:   {'table_schema'} (hidden)\n"
        )


@dataclass
class DataIngestionConfig:
    root_dir: Path
    raw_data_filepath: Path
    dvc_raw_filepath: Path
    ingested_data_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.raw_data_filepath = Path(self.raw_data_filepath)
        self.dvc_raw_filepath = Path(self.dvc_raw_filepath)
        self.ingested_data_filepath = Path(self.ingested_data_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Ingestion Config:\n"
            f"  - Root Dir:           '{self.root_dir.as_posix()}'\n"
            f"  - Raw Data Path:      '{self.raw_data_filepath.as_posix()}'\n"
            f"  - DVC Raw Path:       '{self.dvc_raw_filepath.as_posix()}'\n"
            f"  - Ingested Data Path: '{self.ingested_data_filepath.as_posix()}'\n"
        )


@dataclass
class DataValidationConfig:
    root_dir: Path
    validated_filepath: Path
    dvc_validated_filepath: Path

    schema: ConfigBox
    report_template: ConfigBox
    validation_params: ConfigBox

    missing_report_filepath: Path
    duplicates_report_filepath: Path
    drift_report_filepath: Path
    validation_report_filepath: Path
    categorical_report_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.validated_filepath = Path(self.validated_filepath)
        self.dvc_validated_filepath = Path(self.dvc_validated_filepath)

        self.missing_report_filepath = Path(self.missing_report_filepath)
        self.duplicates_report_filepath = Path(self.duplicates_report_filepath)
        self.drift_report_filepath = Path(self.drift_report_filepath)
        self.validation_report_filepath = Path(self.validation_report_filepath)
        self.categorical_report_filepath = Path(self.categorical_report_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Validation Config:\n"
            f"  - Root Dir:                '{self.root_dir.as_posix()}'\n"
            f"  - Validated CSV:           '{self.validated_filepath.as_posix()}'\n"
            f"  - DVC Validated Path:      '{self.dvc_validated_filepath.as_posix()}'\n"
            f"  - Missing Report:          '{self.missing_report_filepath.as_posix()}'\n"
            f"  - Duplicates Report:       '{self.duplicates_report_filepath.as_posix()}'\n"
            f"  - Drift Report:            '{self.drift_report_filepath.as_posix()}'\n"
            f"  - Categorical Report:      '{self.categorical_report_filepath.as_posix()}'\n"
            f"  - Validation Report:       '{self.validation_report_filepath.as_posix()}'\n"
            f"  - Schema Config:           'schema' (hidden)\n"
            f"  - Report Template:         'template' (hidden)\n"
            f"  - Validation Params:       'params' (hidden)\n"
        )


@dataclass
class DataTransformationConfig:
    root_dir: Path

    # Target and parameters
    target_column: str
    transformation_params: ConfigBox

    # Transformed dataset filepaths
    x_train_filepath: Path
    y_train_filepath: Path
    x_val_filepath: Path
    y_val_filepath: Path
    x_test_filepath: Path
    y_test_filepath: Path

    # DVC-tracked filepaths
    x_train_dvc_filepath: Path
    y_train_dvc_filepath: Path
    x_val_dvc_filepath: Path
    y_val_dvc_filepath: Path
    x_test_dvc_filepath: Path
    y_test_dvc_filepath: Path

    # Preprocessor objects
    x_preprocessor_filepath: Path
    y_preprocessor_filepath: Path

    def __post_init__(self):
        self.root_dir = Path(self.root_dir)
        self.x_train_filepath = Path(self.x_train_filepath)
        self.y_train_filepath = Path(self.y_train_filepath)
        self.x_val_filepath = Path(self.x_val_filepath)
        self.y_val_filepath = Path(self.y_val_filepath)
        self.x_test_filepath = Path(self.x_test_filepath)
        self.y_test_filepath = Path(self.y_test_filepath)

        self.x_train_dvc_filepath = Path(self.x_train_dvc_filepath)
        self.y_train_dvc_filepath = Path(self.y_train_dvc_filepath)
        self.x_val_dvc_filepath = Path(self.x_val_dvc_filepath)
        self.y_val_dvc_filepath = Path(self.y_val_dvc_filepath)
        self.x_test_dvc_filepath = Path(self.x_test_dvc_filepath)
        self.y_test_dvc_filepath = Path(self.y_test_dvc_filepath)

        self.x_preprocessor_filepath = Path(self.x_preprocessor_filepath)
        self.y_preprocessor_filepath = Path(self.y_preprocessor_filepath)

    def __repr__(self) -> str:
        return (
            "\nData Transformation Config:\n"
            f"  - Root Dir:              '{self.root_dir.as_posix()}'\n"
            f"  - Target Column:         '{self.target_column}'\n"
            f"  - X Train:               '{self.x_train_filepath.as_posix()}'\n"
            f"  - Y Train:               '{self.y_train_filepath.as_posix()}'\n"
            f"  - X Val:                 '{self.x_val_filepath.as_posix()}'\n"
            f"  - Y Val:                 '{self.y_val_filepath.as_posix()}'\n"
            f"  - X Test:                '{self.x_test_filepath.as_posix()}'\n"
            f"  - Y Test:                '{self.y_test_filepath.as_posix()}'\n"
            f"  - X Preprocessor:        '{self.x_preprocessor_filepath.as_posix()}'\n"
            f"  - Y Preprocessor:        '{self.y_preprocessor_filepath.as_posix()}'\n"
            f"  - Transformation Params: 'transformation_params' (hidden)\n"
        )

================================================================================
# PY FILE: src\student_performance\exception\__init__.py
================================================================================



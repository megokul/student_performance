--- CODE DUMP | PART 2 of 4 ---


================================================================================
# PY FILE: src\student_performance\components\model_trainer.py
================================================================================

from datetime import datetime, timezone
import os
import importlib
import numpy as np
import optuna
import mlflow
import dagshub
import joblib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import get_scorer
from mlflow import sklearn as mlflow_sklearn

from src.student_performance.entity.config_entity import ModelTrainerConfig
from src.student_performance.entity.artifact_entity import DataTransformationArtifact, ModelTrainerArtifact
from src.student_performance.logging import logger
from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.utils.core import save_to_yaml, save_object, load_array
# from src.student_performance.inference.model import StudentPerformanceModel


class ModelTrainer:
    def __init__(self, config: ModelTrainerConfig, transformation_artifact: DataTransformationArtifact):
        try:
            self.config = config
            self.transformation_artifact = transformation_artifact
            logger.info(f"Initializing ModelTrainer with root_dir={config.root_dir}")
            config.root_dir.mkdir(parents=True, exist_ok=True)

            if config.tracking.mlflow.enabled:
                dagshub.init(
                    repo_owner=os.getenv("DAGSHUB_REPO_OWNER"),
                    repo_name=os.getenv("DAGSHUB_REPO_NAME"),
                    mlflow=True,
                )
                mlflow.set_tracking_uri(config.tracking.tracking_uri)
                mlflow.set_experiment(config.tracking.mlflow.experiment_name)
        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

    def _load_data(self):
        try:
            self.X_train = load_array(self.transformation_artifact.x_train_filepath, "X train")
            self.y_train = load_array(self.transformation_artifact.y_train_filepath, "Y train")
            self.X_val = load_array(self.transformation_artifact.x_val_filepath, "X val")
            self.y_val = load_array(self.transformation_artifact.y_val_filepath, "Y val")
        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

    def _instantiate(self, full_class_string: str, params: dict):
        module_path, class_name = full_class_string.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)(**(params or {}))

    def _optimize_one(self, model_spec: dict):
        model_name = model_spec["name"]
        search_space = model_spec.get("search_space", {})

        def objective(trial):
            sampled = {}
            for name, space in search_space.items():
                if "choices" in space:
                    sampled[name] = trial.suggest_categorical(name, space["choices"])
                else:
                    low, high = space["low"], space["high"]
                    step = space.get("step", 1)
                    log = space.get("log", False)
                    if isinstance(low, int) and isinstance(high, int):
                        sampled[name] = trial.suggest_int(name, low, high, step=step)
                    else:
                        sampled[name] = trial.suggest_float(name, float(low), float(high), log=log)
            clf = self._instantiate(model_name, sampled)
            scores = cross_val_score(clf, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                     scoring=self.config.optimization.scoring, n_jobs=-1)
            return scores.mean()

        study = optuna.create_study(direction=self.config.optimization.direction)
        study.optimize(objective, n_trials=self.config.optimization.n_trials)
        return study.best_trial, study

    def _select_and_tune(self):
        best_result = {"score": -np.inf, "spec": None, "trial": None, "study": None}

        for model_spec in self.config.models:
            if self.config.optimization.enabled:
                trial, study = self._optimize_one(model_spec)
                score = trial.value
            else:
                model = self._instantiate(model_spec["name"], model_spec.get("params", {}))
                score = cross_val_score(model, self.X_train, self.y_train, cv=self.config.optimization.cv_folds,
                                        scoring=self.config.optimization.scoring).mean()
                trial, study = None, None

            if score > best_result["score"]:
                best_result.update(score=score, spec=model_spec, trial=trial, study=study)

        return best_result

    def _train_and_eval(self, model_spec: dict, params: dict):
        clf = self._instantiate(model_spec["name"], params)
        clf.fit(self.X_train, self.y_train)

        train_metrics = {m: get_scorer(m)(clf, self.X_train, self.y_train)
                         for m in self.config.tracking.mlflow.metrics_to_log}
        val_metrics = {m: get_scorer(m)(clf, self.X_val, self.y_val)
                       for m in self.config.tracking.mlflow.metrics_to_log}
        return clf, train_metrics, val_metrics

    def _generate_report(self, best, train_metrics, val_metrics):
        return {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "best_model": best["spec"]["name"].split(".")[-1],
            "best_model_params": best["trial"].params if best["trial"] else best["spec"].get("params", {}),
            "train_metrics": train_metrics,
            "val_metrics": val_metrics,
            "optimization": {
                "enabled": self.config.optimization.enabled,
                "best_trial": best["trial"].number if best["trial"] else None,
                "cv_folds": self.config.optimization.cv_folds,
                "direction": self.config.optimization.direction,
                "mean_score": best["score"]
            }
        }

    def run_training(self) -> ModelTrainerArtifact:
        try:
            logger.info("========== Starting Model Training ==========")
            self._load_data()

            with mlflow.start_run():
                best = self._select_and_tune()
                params = best["trial"].params if best["trial"] else best["spec"].get("params", {})
                model, train_m, val_m = self._train_and_eval(best["spec"], params)

                mlflow.log_params(params)
                for k, v in train_m.items():
                    mlflow.log_metric(f"train_{k}", v)
                for k, v in val_m.items():
                    mlflow.log_metric(f"val_{k}", v)

                # Save raw trained model
                trained_model_dir = self.config.root_dir / "trained_model"
                trained_model_path = trained_model_dir / "model.joblib"
                save_object(model, trained_model_path, "Trained Model")

                # Save inference-ready model (StudentPerformanceModel)
                inference_model = StudentPerformanceModel.from_objects(
                    model=model,
                    x_preprocessor=joblib.load(self.transformation_artifact.x_preprocessor_filepath),
                    y_preprocessor=joblib.load(self.transformation_artifact.y_preprocessor_filepath),
                )
                inference_model_dir = self.config.root_dir / "inference_model"
                inference_model_path = inference_model_dir / "inference_model.joblib"
                save_object(inference_model, inference_model_path, "Inference Model")

                # Save training report
                report_dir = self.config.root_dir / "reports"
                report_path = report_dir / "training_report.yaml"
                save_to_yaml(self._generate_report(best, train_m, val_m), report_path, label="Training Report")

            logger.info("========== Model Training Completed ==========")
            return ModelTrainerArtifact(
                trained_model_filepath=inference_model_path,
                training_report_filepath=report_path,
                x_train_filepath=self.transformation_artifact.x_train_filepath,
                y_train_filepath=self.transformation_artifact.y_train_filepath,
                x_val_filepath=self.transformation_artifact.x_val_filepath,
                y_val_filepath=self.transformation_artifact.y_val_filepath,
                x_test_filepath=self.transformation_artifact.x_test_filepath,
                y_test_filepath=self.transformation_artifact.y_test_filepath,
            )

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\config\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\config\configuration.py
================================================================================

from src.student_performance.constants.constants import (
    CONFIG_ROOT,
    CONFIG_FILENAME,
    PARAMS_FILENAME,
    SCHEMA_FILENAME,
    TEMPLATES_FILENAME,
    LOGS_ROOT,
    ARTIFACTS_ROOT,
    POSTGRES_HANDLER_ROOT,
    DVC_ROOT,
    DVC_RAW_SUBDIR,
    DVC_VALIDATED_SUBDIR,
    DVC_TRANSFORMED_SUBDIR,
    INGEST_ROOT,
    INGEST_RAW_SUBDIR,
    INGEST_INGESTED_SUBDIR,
    VALID_ROOT,
    VALID_VALIDATED_SUBDIR,
    VALID_REPORTS_SUBDIR,
    TRANSFORM_ROOT,
    TRANSFORM_TRAIN_SUBDIR,
    TRANSFORM_TEST_SUBDIR,
    TRANSFORM_VAL_SUBDIR,
    TRANSFORM_PROCESSOR_SUBDIR,
    MODEL_TRAINER_ROOT,
)

from pathlib import Path
import os
from src.student_performance.utils.timestamp import get_utc_timestamp
from src.student_performance.utils.core import read_yaml
from src.student_performance.entity.config_entity import (
    PostgresDBHandlerConfig,
    DataIngestionConfig,
    DataValidationConfig,
    DataTransformationConfig,
    ModelTrainerConfig,
)

class ConfigurationManager:
    _global_timestamp: str = None
    def __init__(self) -> None:
        self._init_artifacts()
        self._load_configs()

    def _init_artifacts(self) -> None:
        if ConfigurationManager._global_timestamp is None:
            ConfigurationManager._global_timestamp = get_utc_timestamp()

        timestamp = ConfigurationManager._global_timestamp
        self.artifacts_root = Path(ARTIFACTS_ROOT) / timestamp
        self.logs_root = Path(LOGS_ROOT) / timestamp

    def _load_configs(self) -> None:
        config_root = Path(CONFIG_ROOT)
        config_filepath = config_root / CONFIG_FILENAME
        params_filepath = config_root / PARAMS_FILENAME
        schema_filepath = config_root / SCHEMA_FILENAME
        templates_filepath = config_root / TEMPLATES_FILENAME

        self.config = read_yaml(config_filepath)
        self.params = read_yaml(params_filepath)
        self.schema = read_yaml(schema_filepath)
        self.templates = read_yaml(templates_filepath)

    def get_postgres_handler_config(self) -> PostgresDBHandlerConfig:

        postgres_config = self.config.postgres_dbhandler
        root_dir = self.artifacts_root / POSTGRES_HANDLER_ROOT
        input_data_filepath = Path(postgres_config.input_data_dir) / postgres_config.input_data_filename
        table_schema = self.schema.table_schema

        return PostgresDBHandlerConfig(
            root_dir=root_dir,
            host=os.getenv("RDS_HOST"),
            port=os.getenv("RDS_PORT"),
            dbname=postgres_config.dbname,
            user=os.getenv("RDS_USER"),
            password=os.getenv("RDS_PASS"),
            table_name=postgres_config.table_name,
            input_data_filepath=input_data_filepath,
            table_schema=table_schema,
        )

    def get_data_ingestion_config(self) -> DataIngestionConfig:
        ingestion_config = self.config.data_ingestion
        raw_data_filename = ingestion_config.raw_data_filename
        ingested_data_filename = ingestion_config.ingested_data_filename

        root_dir = self.artifacts_root / INGEST_ROOT
        raw_data_filepath = root_dir / INGEST_RAW_SUBDIR / raw_data_filename
        dvc_raw_filepath = Path(DVC_ROOT) / DVC_RAW_SUBDIR / raw_data_filename
        ingested_data_filepath = root_dir / INGEST_INGESTED_SUBDIR / ingested_data_filename

        return DataIngestionConfig(
            root_dir=root_dir,
            raw_data_filepath=raw_data_filepath,
            dvc_raw_filepath=dvc_raw_filepath,
            ingested_data_filepath=ingested_data_filepath,
        )

    def get_data_validation_config(self) -> DataValidationConfig:
        validation_config = self.config.data_validation
        schema = self.schema.validation_schema
        report_template = self.templates.validation_report
        validation_params = self.params.validation_params

        validated_data_filename = validation_config.validated_data_filename
        missing_report_filename = validation_config.missing_report_filename
        duplicates_report_filename = validation_config.duplicates_report_filename
        drift_report_filename = validation_config.drift_report_filename
        validation_report_filename = validation_config.validation_report_filename
        categorical_report_filename = validation_config.categorical_report_filename

        root_dir = self.artifacts_root / VALID_ROOT
        validated_filepath = root_dir / VALID_VALIDATED_SUBDIR / validated_data_filename
        missing_report_filepath = root_dir / VALID_REPORTS_SUBDIR / missing_report_filename
        duplicates_report_filepath = root_dir / VALID_REPORTS_SUBDIR / duplicates_report_filename
        drift_report_filepath = root_dir / VALID_REPORTS_SUBDIR / drift_report_filename
        validation_report_filepath = root_dir / VALID_REPORTS_SUBDIR / validation_report_filename
        categorical_report_filepath =  root_dir / VALID_REPORTS_SUBDIR / categorical_report_filename

        dvc_validated_filepath = Path(DVC_ROOT) / DVC_VALIDATED_SUBDIR / validated_data_filename

        return DataValidationConfig(
            root_dir=root_dir,
            validated_filepath=validated_filepath,
            dvc_validated_filepath=dvc_validated_filepath,
            schema=schema,
            report_template=report_template,
            validation_params=validation_params,
            missing_report_filepath=missing_report_filepath,
            duplicates_report_filepath=duplicates_report_filepath,
            drift_report_filepath=drift_report_filepath,
            validation_report_filepath=validation_report_filepath,
            categorical_report_filepath=categorical_report_filepath,
        )

    def get_data_transformation_config(self) -> DataTransformationConfig:
        transformation_config = self.config.data_transformation
        transformation_params = self.params.transformation_params
        output_column = self.schema.target_column

        root_dir = self.artifacts_root / TRANSFORM_ROOT

        # Local paths
        x_train = root_dir / TRANSFORM_TRAIN_SUBDIR / transformation_config.x_train_filename
        y_train = root_dir / TRANSFORM_TRAIN_SUBDIR / transformation_config.y_train_filename
        x_val = root_dir / TRANSFORM_VAL_SUBDIR / transformation_config.x_val_filename
        y_val = root_dir / TRANSFORM_VAL_SUBDIR / transformation_config.y_val_filename
        x_test = root_dir / TRANSFORM_TEST_SUBDIR / transformation_config.x_test_filename
        y_test = root_dir / TRANSFORM_TEST_SUBDIR / transformation_config.y_test_filename

        # DVC-tracked paths
        dvc_root = Path(DVC_ROOT) / DVC_TRANSFORMED_SUBDIR

        x_train_dvc = dvc_root / transformation_config.x_train_filename
        y_train_dvc = dvc_root / transformation_config.y_train_filename
        x_val_dvc = dvc_root / transformation_config.x_val_filename
        y_val_dvc = dvc_root / transformation_config.y_val_filename
        x_test_dvc = dvc_root / transformation_config.x_test_filename
        y_test_dvc = dvc_root / transformation_config.y_test_filename

        # Preprocessor objects
        x_processor_path = root_dir / TRANSFORM_PROCESSOR_SUBDIR / transformation_config.x_preprocessor_filename
        y_processor_path = root_dir / TRANSFORM_PROCESSOR_SUBDIR / transformation_config.y_preprocessor_filename

        return DataTransformationConfig(
            root_dir=root_dir,
            target_column=output_column,
            transformation_params=transformation_params,
            x_train_filepath=x_train,
            y_train_filepath=y_train,
            x_val_filepath=x_val,
            y_val_filepath=y_val,
            x_test_filepath=x_test,
            y_test_filepath=y_test,
            x_train_dvc_filepath=x_train_dvc,
            y_train_dvc_filepath=y_train_dvc,
            x_val_dvc_filepath=x_val_dvc,
            y_val_dvc_filepath=y_val_dvc,
            x_test_dvc_filepath=x_test_dvc,
            y_test_dvc_filepath=y_test_dvc,
            x_preprocessor_filepath=x_processor_path,
            y_preprocessor_filepath=y_processor_path,
        )

    def get_model_trainer_config(self) -> ModelTrainerConfig:
        trainer_config = self.config.model_trainer
        trainer_params = self.params.model_trainer

        root_dir = self.artifacts_root / MODEL_TRAINER_ROOT

        mlflow_cfg = trainer_params.tracking
        mlflow_cfg.tracking_uri = os.getenv("MLFLOW_TRACKING_URI")

        return ModelTrainerConfig(
            root_dir=root_dir,
            trained_model_filepath=trainer_config.trained_model_filename,
            training_report_filepath=trainer_config.training_report_filename,
            models=trainer_params.models,
            optimization=trainer_params.optimization,
            tracking=mlflow_cfg,
        )

================================================================================
# PY FILE: src\student_performance\constants\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\constants\constants.py
================================================================================

CONFIG_ROOT = "config"
CONFIG_FILENAME = "config.yaml"
PARAMS_FILENAME = "params.yaml"
SCHEMA_FILENAME = "schema.yaml"
TEMPLATES_FILENAME = "templates.yaml"

LOGS_ROOT = "logs"

ARTIFACTS_ROOT = "artifacts"

POSTGRES_HANDLER_ROOT = "mongo_handler"
INGEST_ROOT = "data_ingestion"
INGEST_RAW_SUBDIR = "raw_data"
INGEST_INGESTED_SUBDIR = "ingested_data"

DVC_ROOT = "data"
DVC_RAW_SUBDIR = "raw"
DVC_VALIDATED_SUBDIR = "validated"
DVC_TRANSFORMED_SUBDIR = "transformed"

VALID_ROOT = "data_validation"
VALID_VALIDATED_SUBDIR = "validated"
VALID_REPORTS_SUBDIR = "reports"

X_TRAIN_LABEL = "X_train"
Y_TRAIN_LABEL = "y_train"
X_VAL_LABEL = "X_val"
Y_VAL_LABEL = "y_val"
X_TEST_LABEL = "X_test"
Y_TEST_LABEL = "y_test"

TRANSFORM_ROOT = "data_transformation"
TRANSFORM_TRAIN_SUBDIR = "train"
TRANSFORM_TEST_SUBDIR = "test"
TRANSFORM_VAL_SUBDIR = "val"
TRANSFORM_PROCESSOR_SUBDIR = "data_processor"

MODEL_TRAINER_ROOT = "model_trainer"
MODEL_MODEL_SUBDIR = "model"
MODEL_REPORTS_SUBDIR = "reports"

================================================================================
# PY FILE: src\student_performance\data_processors\__init__.py
================================================================================



================================================================================
# PY FILE: src\student_performance\data_processors\column_math_factory.py
================================================================================

from typing import Literal
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from ensure import ensure_annotations

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


ColumnMathOperation = Literal[
    "add", "subtract", "multiply", "divide",
    "mean_of_columns", "sqrt", "square", "power",
]


class ColumnMathFactory(BaseEstimator, TransformerMixin):
    """
    Transformer to apply mathematical operations to specified columns
    and create a new output column.
    """

    def __init__(
        self,
        columns: list[str],
        operation: ColumnMathOperation,
        output_column: str,
    ) -> None:
        self.columns = columns
        self.operation = operation.lower()
        self.output_column = output_column

    def fit(self, X: pd.DataFrame, y: pd.Series = None) -> "ColumnMathFactory":
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        try:
            df = X.copy()

            if self.operation == "add":
                df[self.output_column] = df[self.columns].sum(axis=1)

            elif self.operation == "subtract":
                df[self.output_column] = df[self.columns[0]]
                for col in self.columns[1:]:
                    df[self.output_column] -= df[col]

            elif self.operation == "multiply":
                df[self.output_column] = df[self.columns].prod(axis=1)

            elif self.operation == "divide":
                df[self.output_column] = df[self.columns[0]]
                for col in self.columns[1:]:
                    df[self.output_column] /= df[col]

            elif self.operation == "mean_of_columns":
                df[self.output_column] = df[self.columns].mean(axis=1)

            else:
                raise ValueError(f"Unsupported operation: {self.operation}")

            logger.info(
                "Applied '%s' operation on columns: %s → '%s'",
                self.operation,
                self.columns,
                self.output_column
            )

            return df

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\data_processors\encoder_factory.py
================================================================================

# FILE: src/student_performance/data_processors/encoder_factory.py

from typing import Optional
from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
import pandas as pd

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger

from sklearn.compose import ColumnTransformer

class EncoderFactory:
    _SUPPORTED_METHODS = {
        "one_hot": OneHotEncoder,
        "ordinal": OrdinalEncoder,
    }

    @staticmethod
    @ensure_annotations
    def get_encoder_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False,
    ) -> Pipeline:
        try:
            if method not in EncoderFactory._SUPPORTED_METHODS:
                raise ValueError(f"Unsupported encoding method: {method}")

            encoder_class = EncoderFactory._SUPPORTED_METHODS[method]

            # Extract and remove 'columns' from params
            columns = params.pop("columns", None)
            if columns is None:
                raise ValueError("You must specify 'columns' for encoder in params.yaml")

            encoder = encoder_class(**params)

            # Wrap with ColumnTransformer
            column_transformer = ColumnTransformer(
                transformers=[("encoder", encoder, columns)],
                remainder="passthrough"
            )

            return Pipeline(steps=[("column_encoder", column_transformer)])

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\data_processors\imputer_factory.py
================================================================================

from typing import Optional
from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.impute import KNNImputer, SimpleImputer
# explicitly require this experimental feature
from sklearn.experimental import enable_iterative_imputer  # noqa
# now you can import normally from impute
from sklearn.impute import IterativeImputer

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class ImputerFactory:
    """
    Factory to build imputation pipelines for numerical data.
    """

    _SUPPORTED_METHODS = {
        "knn": KNNImputer,
        "simple": SimpleImputer,
        "iterative": IterativeImputer,
    }

    @staticmethod
    @ensure_annotations
    def get_imputer_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False
    ) -> Pipeline:
        try:
            if method == "custom":
                if not params or "custom_callable" not in params:
                    raise ValueError("Custom imputer requires a 'custom_callable' in params.")
                imputer = params["custom_callable"]()
            else:
                ImputerClass = ImputerFactory._SUPPORTED_METHODS.get(method)
                if not ImputerClass:
                    raise ValueError(f"Unsupported imputation method: {method}")
                imputer = ImputerClass(**(params or {}))

            return Pipeline(steps=[("imputer", imputer)])

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\data_processors\preprocessor_builder.py
================================================================================

from typing import Optional, Dict, Tuple
from sklearn.pipeline import Pipeline
from box import ConfigBox

from src.student_performance.data_processors.imputer_factory import ImputerFactory
from src.student_performance.data_processors.scaler_factory import ScalerFactory
from src.student_performance.data_processors.encoder_factory import EncoderFactory
from src.student_performance.data_processors.column_math_factory import ColumnMathFactory
from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class PreprocessorBuilder:
    """
    Dynamically builds X and Y preprocessing pipelines using configurable steps and methods.

    Supported YAML step keys:
        - imputation      → knn, simple, iterative (via ImputerFactory)
        - standardization → standard_scaler, minmax_scaler, robust_scaler (via ScalerFactory)
        - encoding        → one_hot, ordinal (via EncoderFactory)
        - column_math     → mean, add, power, etc. (via ColumnMathFactory)
    """

    @staticmethod
    def _build_column_math(method: str, params: Dict):
        try:
            return ColumnMathFactory(
                columns=params["input_column"],
                operation=method,
                output_column=params["output_column"],
            )
        except KeyError as e:
            raise ValueError(f"Missing required parameter for column_math: {e}") from e

    STEP_BUILDERS = {
        "imputation": ImputerFactory.get_imputer_pipeline,
        "standardization": ScalerFactory.get_scaler_pipeline,
        "encoding": EncoderFactory.get_encoder_pipeline,
        "column_math": _build_column_math.__func__,
    }

    def __init__(self, steps: Optional[Dict] = None, methods: Optional[Dict] = None) -> None:
        """
        Args:
            steps (Dict): Ordered step names per section (e.g., {"x": ["imputation", "encoding"]})
            methods (Dict): Method config per section (e.g., {"x": {"imputation": {...}}})
        """
        self.steps = steps or {}
        self.methods = methods or {}

    def _build_pipeline(self, section: str) -> Pipeline:
        """
        Build a scikit-learn pipeline for a given section ("x" or "y").

        Args:
            section (str): Section name ('x' or 'y')

        Returns:
            Pipeline: A scikit-learn Pipeline object
        """
        try:
            pipeline_steps = []
            step_list = self.steps.get(section, [])
            section_methods = self.methods.get(section, {})

            for step_name in step_list:
                step_config = section_methods.get(step_name, {})

                # Skip if explicitly set to "none"
                if not step_config or (
                    isinstance(step_config, str) and step_config.lower() == "none"
                ):
                    logger.info(
                        f"Skipping '{step_name}' step in section '{section}' as it is set to 'none'."
                    )
                    continue

                builder_fn = self.STEP_BUILDERS.get(step_name)
                if not builder_fn:
                    raise ValueError(f"Unsupported preprocessing step: '{step_name}'")

                if isinstance(step_config, (dict, ConfigBox)):
                    method_name = step_config.get("method")
                    step_params = {k: v for k, v in step_config.items() if k != "method"}
                else:
                    method_name = step_config
                    step_params = {}

                step_object = builder_fn(method_name, step_params)
                pipeline_steps.append((step_name, step_object))

            return Pipeline(pipeline_steps)

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

    def build(self) -> Tuple[Pipeline, Optional[Pipeline]]:
        """
        Builds preprocessing pipelines for both features (X) and target (Y).

        Returns:
            Tuple[Pipeline, Optional[Pipeline]]: Tuple containing x_pipeline and y_pipeline
        """
        try:
            x_pipeline = self._build_pipeline("x")
            y_pipeline = self._build_pipeline("y")
            return x_pipeline, y_pipeline
        except Exception as e:
            raise StudentPerformanceError(e, logger) from e

================================================================================
# PY FILE: src\student_performance\data_processors\scaler_factory.py
================================================================================

from typing import Optional
from ensure import ensure_annotations
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

from src.student_performance.exception.exception import StudentPerformanceError
from src.student_performance.logging import logger


class ScalerFactory:
    """
    Factory to build scaling pipelines for numerical features.
    """

    _SUPPORTED_METHODS = {
        "standard_scaler": StandardScaler,
        "minmax": MinMaxScaler,
        "robust": RobustScaler,
    }

    @staticmethod
    @ensure_annotations
    def get_scaler_pipeline(
        method: str,
        params: Optional[dict] = None,
        is_target: bool = False
    ) -> Pipeline:
        try:
            if method not in ScalerFactory._SUPPORTED_METHODS:
                raise ValueError(f"Unsupported scaler method: {method}")

            scaler_cls = ScalerFactory._SUPPORTED_METHODS[method]
            scaler = scaler_cls(**(params or {}))

            return Pipeline(steps=[("scaler", scaler)])

        except Exception as e:
            raise StudentPerformanceError(e, logger) from e
